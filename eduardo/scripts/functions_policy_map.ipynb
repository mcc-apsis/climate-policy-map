{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "********** LOADING **********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.io.shapereader as shpreader\n",
    "import pandas as pd\n",
    "import geopy\n",
    "import geopy.distance\n",
    "import numpy as np\n",
    "import shapely.vectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "********** FUNCTIONS FOR CREATING the DENSITY GRID **********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Max's\n",
    "def new_haversine_np(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points\n",
    "    on the earth.\n",
    "\n",
    "    Args:\n",
    "        lon1 (float): Longitude of point 1 (specified in decimal degrees).\n",
    "        lat1 (float): Latitude of point 1 (specified in decimal degrees).\n",
    "        lon2 (float): Longitude of point 2 (specified in decimal degrees).\n",
    "        lat2 (float): Latitude of point 2 (specified in decimal degrees).\n",
    "\n",
    "    Returns:\n",
    "        Distance in kilometers.\n",
    "    \"\"\"\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    dlon = lon2 - lon1[:,None]\n",
    "    dlat = lat2 - lat1[:,None]\n",
    "\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1[:,None]) * np.cos(lat2) * np.sin(dlon/2.0)**2\n",
    "\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    km = 6367 * c\n",
    "    \n",
    "    return km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max's\n",
    "def density_grid(degrees,distance,df):\n",
    "    df_countries = df[df[\"feature_code\"]==\"PCLI\"]\n",
    "    df_places = df[df[\"feature_code\"]!=\"PCLI\"]\n",
    "\n",
    "    # linspace(start, stop, number of evenly spaced numbers)\n",
    "    latbins = np.linspace(-90,90, round(180/degrees))\n",
    "    lonbins = np.linspace(-180,180, round(360/degrees))\n",
    "\n",
    "    n = np.zeros((len(latbins),len(lonbins)))\n",
    "    \n",
    "    print(f\"calculating density grid of size: {n.size}\")\n",
    "\n",
    "    for i,lat in enumerate(latbins):\n",
    "        # Calculating the geodesic distance between two points\n",
    "        r = geopy.distance.distance(kilometers=distance)\n",
    "        latp = geopy.Point((lat,135))\n",
    "        # if the latitude is closer than distance to the north pole, then the northern bound should be \n",
    "        # the north pole, not distancekm north of the latitude (which will pass the pole and go south again)\n",
    "        if geopy.distance.great_circle(latp,(90,135)).km < distance:\n",
    "            r_nbound = 90   \n",
    "        else:\n",
    "            r_nbound = r.destination(point=latp,bearing=0).latitude\n",
    "        # Same as above for the south pole\n",
    "        if geopy.distance.great_circle(latp,(-90,135)).km < distance:\n",
    "            r_sbound = -90   \n",
    "        else:\n",
    "            r_sbound = r.destination(point=latp,bearing=180).latitude        \n",
    "\n",
    "        latbound_df = df_places[\n",
    "            (df_places.lat>=r_sbound) &\n",
    "            (df_places.lat<=r_nbound)        \n",
    "        ]\n",
    "\n",
    "        ds = new_haversine_np(latbound_df['lon'], latbound_df['lat'],lonbins,[lat]*len(lonbins))\n",
    "\n",
    "        n[i,:] = np.where(ds<distance,1,0).sum(axis=0)\n",
    "        \n",
    "    print(\"done\")\n",
    "    \n",
    "    shpfilename = shpreader.natural_earth(resolution='110m',\n",
    "                                          category='cultural',\n",
    "                                          name='admin_0_countries')\n",
    "    reader = shpreader.Reader(shpfilename)\n",
    "    yv, xv = np.meshgrid(latbins, lonbins)\n",
    "\n",
    "    for country in reader.records():\n",
    "        incountry = shapely.vectorized.contains(country.geometry,xv,yv)\n",
    "        idx = np.argwhere(incountry==True)\n",
    "        ndots = idx.size/2\n",
    "        cdf = df_countries[df_countries[\"country_predicted\"]==country.attributes[\"SU_A3\"]]\n",
    "        for point in idx:\n",
    "            n[point[1],point[0]] += cdf.shape[0]/ndots\n",
    "\n",
    "    return latbins, lonbins, n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "********** DATA MANIPULATION **********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_list_of_predicted_categories(df):\n",
    "    \"\"\"Creates a list with all categories in the dataframe and a list with categories of the dataframe that have predictions.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input dataframe (combined document ratings and geo data).\n",
    "\n",
    "    Returns:\n",
    "        all_cats: A list with all categories in the dataframe.\n",
    "        pred_cats: A list with categories that have predictions.\n",
    "    \"\"\"\n",
    "    data = df.copy()\n",
    "    \n",
    "    all_cats = [c for c  in data.columns if (\"-\" in c) and (\"prediction\" not in c) and (\"hidden\" not in c)] # This list contains all variable columns.\n",
    "    preds = [c for c in data.columns if \"prediction\" in c and \"hidden\" not in c] # This list contains all columns that are predictions.\n",
    "    pred_cats = []\n",
    "    for c in all_cats:\n",
    "        for p in preds:\n",
    "            if c in p:\n",
    "                pred_cats.append(c)\n",
    "    return all_cats, pred_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_columns(df, col, col_pred):\n",
    "    \"\"\"Creates a dataframe with a merged human/prediction column.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input dataframe (combined document ratings and geo data).\n",
    "        col (str): Name of the column that contains the variable.\n",
    "        col_pred (str): Name of the column with the predicted variable.\n",
    "\n",
    "    Returns:\n",
    "        A modified dataframe with a column that has the predicted values if the paper was not seen by a human, and the \"human\" values if the paper was seen. The column has the same name of the variable.\n",
    "    \"\"\"\n",
    "    \n",
    "    data = df.copy()\n",
    "    data['human'] = data[col] # This creates a copy of human data under the \"human\" header\n",
    "    data[col] = data[col_pred] # This replaces the column of interest (human data) with predicted data\n",
    "    \n",
    "    i = 0\n",
    "    while(i < len(data)):\n",
    "        if data['seen'].iloc[i] == 1:\n",
    "            data.at[i, col] = data['human'].iloc[i] # This replaces the predicted data, which is under the \"column of interest\" header, with the human data from the copied column.\n",
    "        i = i + 1\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter(df, cat, prob=0.5):\n",
    "    \"\"\"Filters a dataframe based on a variable of interest.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input dataframe (combined document ratings and geo data).\n",
    "        cat (str): Name of the column that contains the variable. 'All' if no filtering is to be done.\n",
    "        prob (float): Minimum probability threshold to consider that a paper is about the variable of interest. Default: 0.5\n",
    "\n",
    "    Returns:\n",
    "        A dataframe that has been filtered according to a specific variable.\n",
    "    \"\"\"\n",
    "    filtered = df.copy()\n",
    "    all_cats, pred_cats = create_list_of_predicted_categories(df)\n",
    "    \n",
    "    if(cat == 'All' or cat == 'all'):\n",
    "        return filtered\n",
    "    elif(cat in pred_cats):\n",
    "        pred = cat + ' - prediction'\n",
    "        filtered = merge_columns(filtered, cat, pred)\n",
    "        filtered = filtered[filtered[cat] >= prob]\n",
    "        return filtered\n",
    "    elif(cat in all_cats):\n",
    "        filtered = filtered[filtered[cat] >= prob]\n",
    "        return filtered\n",
    "    else:\n",
    "        print('Wrong variable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_country(df, country_code, country_col='country_predicted'):\n",
    "    \"\"\" Creates a dataframe of articles about the specified country.\n",
    "\n",
    "        Args:\n",
    "            df (DataFrame): Dataframe with article data.\n",
    "            country_code (str): World Banck country code of the country of interest.\n",
    "            country_col (str): Name of the column with the countries. Default: 'country_predicted'\n",
    "\n",
    "        Returns:\n",
    "            Dataframe with articles about the specified country.\n",
    "\n",
    "    \"\"\"    \n",
    "    data = df[df[country_col]==country_code].sort_values(by=['id']).reset_index(drop=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mislabeled_rows(df, country_code, associations, country_col='country_predicted'):\n",
    "    \"\"\"\n",
    "        Remove rows that correspond to most probably mislocalised articles due to specific expressions, like \"Paris Agreement\".\n",
    "\n",
    "        Args:\n",
    "            df (DataFrame): Dataframe to correct for possible mislocalisations.\n",
    "            country_code (str): Country code as used by the World Bank.\n",
    "            associations (list): List of expressions that may generate mislabeling.\n",
    "            country_col (str): Column with country codes.\n",
    "\n",
    "        Returns:\n",
    "            Dataframe without rows that most possibly are mislabeled.\n",
    "    \"\"\"\n",
    "    # Country-exclusive data    \n",
    "    dat_country = filter_by_country(df, country_code, country_col)\n",
    "    # Removing country data from the total\n",
    "    data = df[df[country_col]!=country_code]\n",
    "    \n",
    "    inds_to_drop = []\n",
    "    for exp in associations:\n",
    "        for i in range(0, len(dat_country)):\n",
    "            title = dat_country.iloc[i]['title']\n",
    "            if exp in title:\n",
    "                inds_to_drop.append(i)\n",
    "            else:\n",
    "                abstract = dat_country.iloc[i]['content']\n",
    "                if exp in abstract:\n",
    "                    inds_to_drop.append(i)\n",
    "    dat_country = dat_country.drop(index=inds_to_drop)\n",
    "    data = pd.concat([data, dat_country])\n",
    "    data = data.sort_values(by=['id']).reset_index(drop=True)\n",
    "    return data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mismatch_dictionary(df, df2, country_column='aff_country'):\n",
    "    \"\"\"\n",
    "       Creates a dictionary of correct matches between countries in a list and World Bank country codes.\n",
    "\n",
    "       Args:\n",
    "        df (DataFrame): Dataframe with a column of countries.\n",
    "        df2 (DataFrame): Country classification table (data/country_classification.xls')\n",
    "        country_column (str): Name of column with country names.\n",
    "    \"\"\"\n",
    "    mismatches = []\n",
    "    for country in df[country_column]:\n",
    "        if country not in list(df2['Economy']):\n",
    "           mismatches.append(country)\n",
    "    mismatches = pd.unique(mismatches)\n",
    "    # Manually establishing pairs\n",
    "    mismatch_pairs = {}\n",
    "    for mismatch in mismatches:\n",
    "        print('Introduce country code for '+mismatch+':')\n",
    "        code = input()\n",
    "        mismatch_pairs[mismatch] = code\n",
    "    # Adding all pairs\n",
    "    i = 0\n",
    "    while i < len(df2):\n",
    "        country = df2['Economy'][i]\n",
    "        code = df2['Code'][i]\n",
    "        mismatch_pairs[country] = code\n",
    "        i = i+1\n",
    "\n",
    "    f = open(\"../data/complete_country_codes.txt\",\"w\")\n",
    "    f.write( str(mismatch_pairs) )\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_human_and_predicted(df, cols):\n",
    "    \"\"\"\n",
    "    Merges the human and predicted columns of a variable.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame on which to merge human and predicted columns.\n",
    "        cols (list): List of columns that have an analogous \" - predicted\" column. See create_list_of_predicted_categories(data).\n",
    "\n",
    "    Return:\n",
    "        A DataFrame where the human column of each variable is now the human column, unless it was not seen, in which case it would be the predicted value.\n",
    "    \"\"\"\n",
    "    data = df.copy()\n",
    "    for col in cols:\n",
    "        data = merge_columns(data, col, col+' - prediction')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_by_col_and_row(df):\n",
    "    \"\"\"\n",
    "        Normalises a dataframe by row and, separately, by column.\n",
    "        \n",
    "        Args:\n",
    "            df (DataFrame): Dataframe to normalise.\n",
    "        \n",
    "        Returns:\n",
    "            Dataframe normalised by column. NAs filled with zeroes.\n",
    "            Dataframe normalised by row. NAs filled with zeroes.\n",
    "    \"\"\"\n",
    "    col_norm = df/df.sum(axis=0)\n",
    "    row_norm = df.div(df.sum(axis=1), axis=0)\n",
    "    col_norm.fillna(value=0, inplace=True)\n",
    "    row_norm.fillna(value=0, inplace=True)\n",
    "    return col_norm, row_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "********** PLOTTING **********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_map(cat, degrees, distance, df, file_name, dpi=150, width=8, height=5):\n",
    "    \"\"\"Creates a map based on a variable of interest and saves it in a file.\n",
    "\n",
    "    Args:\n",
    "        cat (str): Variable of interest. \"All\" if all papers are to be mapped.\n",
    "        degrees (float): Number of degrees.\n",
    "        distance (int): Distance of the clusters.\n",
    "        df (DataFrame): The input dataframe (combined document ratings and geo data).\n",
    "        file_name (str): Name of the plot file.\n",
    "        dpi (int): Resolution (dots per inch).\n",
    "        width (int): Width of the plot. Default: 8.\n",
    "        height (int): Height of the plot. Default: 5.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(dpi=dpi, figsize=(width, height))\n",
    "    p = ccrs.Mollweide()\n",
    "    ax = plt.axes(projection=p)\n",
    "    ax.set_global()\n",
    "    ax.coastlines(lw=0.1)\n",
    "\n",
    "    filtered = filter(df, cat)\n",
    "    latbins, lonbins, n = density_grid(degrees,distance,filtered)\n",
    "\n",
    "    vm = n[~np.isnan(n)].max()\n",
    "    n[n == 0] = np.nan\n",
    "\n",
    "    pcm = plt.pcolormesh( \n",
    "        lonbins, latbins, n,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        norm=mpl.colors.LogNorm(vmin=1, vmax=vm),\n",
    "        alpha=0.5,\n",
    "        cmap=\"YlGnBu\"\n",
    "    )\n",
    "\n",
    "    fig.colorbar(pcm)\n",
    "    title = cat.split(\" - \")[1]\n",
    "    ax.set_title(title)\n",
    "    filename = '../plots/density_maps/' + file_name + '.pdf'\n",
    "    plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_maps(df):\n",
    "    \"\"\"Plots all variables in a dataframe.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input dataframe (combined document ratings and geo data).\n",
    "    \"\"\"\n",
    "    all_cats, pred_cats = create_list_of_predicted_categories(df)\n",
    "    for counter, c in enumerate(all_cats):\n",
    "        file_name = c.split(' - ')[1].replace(\" \",\"_\").replace(\"/\", \"_\")\n",
    "        print(file_name)\n",
    "        plot_map(c, 1, 100, df, file_name)\n",
    "        print(str((counter/len(all_cats))*100) + '%')"
   ]
  }
 ]
}